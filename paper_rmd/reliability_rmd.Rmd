---
title: "A reliability study on the ACLEW corpora"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(ggplot2)
evaldir_rel=c("../reliability_evaluations/")
```

```{r cars, include=FALSE}
## Load the data
# Load identification error rate report
ider <- read.table(file=paste0(evaldir_rel,"metrics/ider_report.csv"), header=TRUE,sep=",")
rownames(ider) <- ider$item
ider["corpora"] = substr(ider$item, 1, 3)
ider["TOTAL", "corpora"] = NA
ider <- subset(ider, select = -item)
colnames(ider) = c("ider%", "total", "correct", "correct%", "fa", "fa%", "miss", "miss%", "conf", "conf%", "corpora")

data = ider[! is.na(ider$corpora),]
corpus = c("ALL", "BER", "ROW", "WAR", "TSE", "ROS", "SOD")
per_corpora = data.frame(corpora=corpus) 

for(corpora in corpus){
  if(corpora=="ALL"){
    ider_tot = (sum(data$fa) + sum(data$miss) + sum(data$conf))/ sum(data$total) * 100
    per_corpora[per_corpora["corpora"] == "ALL", "ider"] = ider_tot
  } else {
    subset = data[data["corpora"] == corpora,]
    ider_metric = (sum(subset$fa) + sum(subset$miss) + sum(subset$conf))/ sum(subset$total) * 100
    per_corpora[per_corpora["corpora"] == corpora, "ider"] = ider_metric
  }
}

per_corpora = per_corpora[order(per_corpora$ider),]
```

## Experimental set-up

For studying the reliability of human annotators, and get a sense of their level of agreement, we asked to a second person to annonate a 1-mn long chunk from the daylong recording of each child. The purpose being to compare metrics obtained by these two annotators. This reliability study has been performed on SOD, WAR, ROS, TSE, ROW and BER corpus. We have 10 children by sub-corpora, for a total of 60 chunks of 1 minute.

We mapped all the labels into :

- CHI : for the key-child, the one wearing the recording device
- OCH : for other chidren
- MAL : for male speakers
- FEM : for female speakers
- OVL : for overlap
- SIL : for silence

## Performances metrics

### Identification Error Rate as a overall performance measure
One might want compare the level agreement of the two annotators as a function of the identification error rate. As a reminder, the identification error rate is computed as follow :
$$ \text{identification error rate} = \dfrac{\text{false alarm} + \text{miss} + \text{confusion}}{\text{total}} $$
where : 

$\text{false alarm}$ is the duration of non-speech incorrectly classified as speech

$\text{miss}$ is the duration of speech incorrectly classified as non-speech

$\text{confusion}$ is the duration of speaker confusion (agreements on the fact that there's speech, but disagreement on the talker identity).

$\text{speech}$ is the duration of speaker confusion (agreements on the fact that there's speech, but disagreement on the talker identity).

The two annotators obtained an identification error rate of `r toString(ider["TOTAL","ider%"])`% shared amongst a false alarm rate of `r toString(ider["TOTAL","fa%"])`%, a miss rate of `r toString(ider["TOTAL","miss%"])`% and a confusion of `r toString(ider["TOTAL","conf%"])`%.

Here's the per corpora identification error rate : 

```{r, echo=FALSE}
kable(per_corpora, row.names=FALSE) %>%
  kable_styling()
```

#### Best cases

The three best cases, for which the agreement was the highest were : 

```{r, echo=FALSE}
kable(ider[order(ider["ider%"]),][1:3,]) %>%
  kable_styling()
```

#### Worst cases

The three worst cases, for which the disagreement was the highest were : 

```{r, echo=FALSE}
kable(ider[order(-ider["ider%"]),][1:3,]) %>%
  kable_styling()
```

### Detection Error Rate as a per-class performance measure

One can have a look at the per-class detection error rate defined as : 

$$\text{detection error rate} = \dfrac{\text{false alarm} + \text{miss}}{\text{total}}$$

```{r, echo=FALSE}
classes = c("CHI", "ELE", "FEM", "MAL", "OCH", "OVL")
per_corpora_per_class = data.frame(matrix(vector(), 6, 7,
                                          dimnames=list(classes, corpus)),
                                   stringsAsFactors=F)

for (class in classes){
  data_class = read.table(file=paste0(evaldir_rel, "/metrics/only_",class,"_deter_report.csv"), 
                          header=TRUE, sep=",")
  rownames(data_class) <- data_class$item
  data_class["corpora"] = substr(data_class$item, 1, 3)
  data_class = head(data_class, -1)
  data_class <- subset(data_class, select = -item)
  
  for (corpora in corpus){
    if (corpora=="ALL") {
      subset = data_class
      deter_score =  (sum(subset$fa) + sum(subset$miss))/ sum(subset$total) * 100
      per_corpora_per_class[class,corpora] = deter_score
    } else {
      subset = data_class[data_class["corpora"] == corpora,]
      deter_score =  (sum(subset$fa) + sum(subset$miss))/ sum(subset$total) * 100
      per_corpora_per_class[class,corpora] = deter_score
    }
  }
}
per_corpora_per_class = per_corpora_per_class[order(per_corpora_per_class$ALL),]
# chi <- read.table(file="reliability_evaluations/metrics/only_CHI_deter_report.csv", header=TRUE, sep=",")
# ele <- read.table(file="reliability_evaluations/metrics/only_ELE_deter_report.csv", header=TRUE, sep=",")
# fem <- read.table(file="reliability_evaluations/metrics/only_FEM_deter_report.csv", header=TRUE, sep=",")
# mal <- read.table(file="reliability_evaluations/metrics/only_MAL_deter_report.csv", header=TRUE, sep=",")
# och <- read.table(file="reliability_evaluations/metrics/only_OCH_deter_report.csv", header=TRUE, sep=",")
# ovl <- read.table(file="reliability_evaluations/metrics/only_OVL_deter_report.csv", header=TRUE, sep=",")
# rownames(chi) <- chi$item
# rownames(ele) <- ele$item
# rownames(fem) <- fem$item
# rownames(mal) <- mal$item
# rownames(och) <- och$item
# rownames(ovl) <- ovl$item
# tot_chi <- chi["TOTAL", "detection.error.rate.."]
# tot_ele <- ele["TOTAL", "detection.error.rate.."]
# tot_fem <- fem["TOTAL", "detection.error.rate.."]
# tot_mal <- mal["TOTAL", "detection.error.rate.."]
# tot_och <- och["TOTAL", "detection.error.rate.."]
# tot_ovl <- ovl["TOTAL", "detection.error.rate.."]
# 
# tot <- c(tot_chi,tot_ele,tot_fem,tot_mal, tot_och,tot_ovl)
# classes <- c("CHI", "ELE", "FEM", "MAL", "OCH","OVL")
# deter <- data.frame(class=classes, total=tot)
# deter <- deter[order(deter$tot),]
# 
kable(per_corpora_per_class) %>%
  kable_styling()

chi <- read.table(file=paste0(evaldir_rel,"/metrics/only_CHI_deter_report.csv"), header=TRUE, sep=",")
rownames(chi) <- chi$item
tot_chi <- chi["TOTAL", "detection.error.rate.."]

```

With no surprise, there's a high disagreement for classes such as the OVL one for which it is harder to tell when it starts and it ends exactly. The highest agreement is obtained for the CHI class for which the two annotators obtainted a detection error rate of `r tot_chi`%

#### Best agreement for the CHI class

The three best cases, for which the agreement on the CHI class was the highest were : 

```{r, echo=FALSE}
chi <- subset(chi, select = -item)
colnames(chi) <- c("deter%", "total", "fa", "fa%", "miss", "miss%")
kable(chi[order(chi["deter%"]),][1:3,]) %>%
  kable_styling()
```

#### Worst agreement for the CHI class

The three worst cases, for which the disagreement on the CHI class was the highest were : 

```{r, echo=FALSE}
kable(chi[order(-chi["deter%"]),][1:3,]) %>%
  kable_styling()
```

### Precision/Recall as a per-class performance measure

As illustrated by the two tables shown above, the detection error rate (like the identification error rate) can be tricky to interpret when little speech is contained in the chunk. Indeed, in that particular case, the denominator is close to 0 (or equal to 0 if there's no speech), hence pumping up the measure. One might be more familiar with metrics such as the precision and the recall defined as :

$$ \text{precision} = \dfrac{tp}{tp+fp}$$
$$ \text{recall} = \dfrac{tp}{tp+fn}$$

where :
$tp$ is the duration of true positive (e.g. speech classified as speech)

$fp$ is the duration of false positive (e.g. non-speech classified as speech)

$fn$ is the duration of false negative (e.g speech classified as non-speech)

```{r, echo=FALSE}
chip <- read.table(file=paste0(evaldir_rel,"/metrics/only_CHI_precision_report.csv"), header=TRUE, sep=",")
elep <- read.table(file=paste0(evaldir_rel,"/metrics/only_ELE_precision_report.csv"), header=TRUE, sep=",")
femp <- read.table(file=paste0(evaldir_rel,"/metrics/only_FEM_precision_report.csv"), header=TRUE, sep=",")
malp <- read.table(file=paste0(evaldir_rel,"/metrics/only_MAL_precision_report.csv"), header=TRUE, sep=",")
ochp <- read.table(file=paste0(evaldir_rel,"/metrics/only_OCH_precision_report.csv"), header=TRUE, sep=",")
ovlp <- read.table(file=paste0(evaldir_rel,"/metrics/only_OVL_precision_report.csv"), header=TRUE, sep=",")
rownames(chip) <- chip$item
rownames(elep) <- elep$item
rownames(femp) <- femp$item
rownames(malp) <- malp$item
rownames(ochp) <- ochp$item
rownames(ovlp) <- ovlp$item
tot_chip <- chip["TOTAL", "detection.precision.."]
tot_elep <- elep["TOTAL", "detection.precision.."]
tot_femp <- femp["TOTAL", "detection.precision.."]
tot_malp <- malp["TOTAL", "detection.precision.."]
tot_ochp <- ochp["TOTAL", "detection.precision.."]
tot_ovlp <- ovlp["TOTAL", "detection.precision.."]

chir <- read.table(file=paste0(evaldir_rel,"/metrics/only_CHI_recall_report.csv"), header=TRUE, sep=",")
eler <- read.table(file=paste0(evaldir_rel,"/metrics/only_ELE_recall_report.csv"), header=TRUE, sep=",")
femr <- read.table(file=paste0(evaldir_rel,"/metrics/only_FEM_recall_report.csv"), header=TRUE, sep=",")
malr <- read.table(file=paste0(evaldir_rel,"/metrics/only_MAL_recall_report.csv"), header=TRUE, sep=",")
ochr <- read.table(file=paste0(evaldir_rel,"/metrics/only_OCH_recall_report.csv"), header=TRUE, sep=",")
ovlr <- read.table(file=paste0(evaldir_rel,"/metrics/only_OVL_recall_report.csv"), header=TRUE, sep=",")
rownames(chir) <- chip$item
rownames(eler) <- elep$item
rownames(femr) <- femp$item
rownames(malr) <- malp$item
rownames(ochr) <- ochp$item
rownames(ovlr) <- ovlp$item
tot_chir <- chir["TOTAL", "detection.recall.."]
tot_eler <- eler["TOTAL", "detection.recall.."]
tot_femr <- femr["TOTAL", "detection.recall.."]
tot_malr <- malr["TOTAL", "detection.recall.."]
tot_ochr <- ochr["TOTAL", "detection.recall.."]
tot_ovlr <- ovlr["TOTAL", "detection.recall.."]

totp <- c(tot_chip,tot_elep,tot_femp,tot_malp, tot_ochp,tot_ovlp)
totr <- c(tot_chir,tot_eler,tot_femr,tot_malr, tot_ochr,tot_ovlr)
classes <- c("CHI", "ELE", "FEM", "MAL", "OCH","OVL")
prerec <- data.frame(class=classes, precision=totp, recall=totr)
prerec <- prerec[order(-prerec$precision),]

kable(prerec, row.names=FALSE) %>%
  kable_styling()
```


```{r ggprec-fig, echo=FALSE,fig.cap="Precision (left) and recall (right) confusion matrices on all of the corpus"}
library(scales)
library(gridExtra)
confmat = read.table(file=paste0(evaldir_rel,"/all_cm.txt"), header=TRUE, sep=" ")
confmat$Other=confmat$SIL+confmat$OVL+confmat$ELE
confmat["Other",]<-confmat["SIL",]+confmat["OVL",]+confmat["ELE",]
labels = c("CHI", "OCH", "FEM", "MAL", "Other")
confmat = confmat[labels,labels]
dodiv=function(x) x/sum(x, na.rm=T)

all=confmat
prop_cat=data.frame(apply(all,2,dodiv)*100) #generates precision because columns
#colSums(prop_cat)
stack(all)->stall
colnames(stall)<-c("n","LENA")
stall$human=factor(rownames(all),levels = rownames(all))
stall$pr=stack(prop_cat)$values

prop_cat=data.frame(apply(all,1,dodiv)*100) #generates recall because rows

stall$rec=stack(prop_cat)$values

plot1 = ggplot(data = stall, mapping = aes(y = stall$human, x=stall$LENA)) +
 geom_tile(aes(fill= rescale(stall$pr)), colour = "white") +
  geom_text(aes(label = paste(round(stall$pr),"%")), vjust = -1,size=2) +
  geom_text(aes(label = stall$n), vjust = 1,size=2) +
  scale_fill_gradient(low = "white", high = "red", name = "Proportion") +
  xlab("Human 1") + ylab("Human 2") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
plot2 = ggplot(data = stall, mapping = aes(y = stall$human, x=stall$LENA)) + 
  geom_tile(aes(fill= rescale(stall$rec)), colour = "white") +
  geom_text(aes(label = paste(round(stall$rec),"%")), vjust = -1,size=2) + 
  geom_text(aes(label = stall$n), vjust = 1,size=2) + 
  scale_fill_gradient(low = "white", high = "green", name = "Proportion") +
  xlab("Human 1") + ylab("Human 2") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

grid.arrange(plot1, plot2, ncol=2)
```

```{r ggprec-fig2, echo=FALSE,fig.cap="Precision (left) and recall (right) confusion matrices per corpora"}
library(scales)
library(grid)
library(gridExtra)

corpus = c("BER", "ROW", "WAR", "TSE", "ROS", "SOD")
for(corpora in corpus){
  confmat = read.table(file=paste0(evaldir_rel, "/",corpora,"_cm.txt"), header=TRUE, sep=" ")
  confmat$Other=confmat$SIL+confmat$OVL+confmat$ELE
  confmat["Other",]<-confmat["SIL",]+confmat["OVL",]+confmat["ELE",]
  labels = c("CHI", "OCH", "FEM", "MAL", "Other")
  confmat = confmat[labels,labels]
  dodiv=function(x) x/sum(x, na.rm=T)
  all=confmat
  prop_cat=data.frame(apply(all,2,dodiv)*100) #generates precision because columns
  #colSums(prop_cat)
  stack(all)->stall
  colnames(stall)<-c("n","LENA")
  stall$human=factor(rownames(all),levels = rownames(all))
  stall$pr=stack(prop_cat)$values
  
  prop_cat=data.frame(apply(all,1,dodiv)*100) #generates recall because rows
  
  stall$rec=stack(prop_cat)$values
  
  plot1 = ggplot(data = stall, mapping = aes(y = stall$human, x=stall$LENA)) +
   geom_tile(aes(fill= rescale(stall$pr)), colour = "white") +
    geom_text(aes(label = paste(round(stall$pr),"%")), vjust = -1,size=2) +
    geom_text(aes(label = stall$n), vjust = 1,size=2) +
    scale_fill_gradient(low = "white", high = "red", name = "Proportion") +
    xlab("Human 1") + ylab("Human 2") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
  plot2 = ggplot(data = stall, mapping = aes(y = stall$human, x=stall$LENA)) + 
    geom_tile(aes(fill= rescale(stall$rec)), colour = "white") +
    geom_text(aes(label = paste(round(stall$rec),"%")), vjust = -1,size=2) + 
    geom_text(aes(label = stall$n), vjust = 1,size=2) + 
    scale_fill_gradient(low = "white", high = "green", name = "Proportion") +
    xlab("Human 1") + ylab("Human 2") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
  grid.arrange(plot1, plot2, ncol=2, top = textGrob(paste0("Precision/Recall on ", corpora),gp=gpar(fontsize=20,font=3)))
}
```

## Cohen's kappa

As an other measure of the level of agreement between the two annotators, we propose the use of the Cohen's kappa measure, defined as follow : 

$$ \kappa = \dfrac{\text{Pr}(\alpha) - \text{Pr}(e)}{1-\text{Pr}(e)} $$

where: 
$\text{Pr}(\alpha)$ is the relative agreement between the annotators.
$\text{Pr}(\alpha)$ is the probability of a random agreement on a given frame.

If both annotators fully agree, $\kappa = 1$, if they fully disagree (or agree randomly), $\kappa = 0$

```{r kappa, echo=FALSE, message=FALSE}
library(psych, pos = .Machine$integer.max)

corpus = c("all", "BER", "ROW", "WAR", "TSE", "ROS", "SOD")
cohen = data.frame(corpora=corpus)

for(corpora in corpus){
  confmat = read.table(file=paste0(evaldir_rel, "/", corpora, "_cm.txt"), header=TRUE, sep=" ")
  confmat$Other=confmat$SIL+confmat$OVL+confmat$ELE
  confmat["Other",]<-confmat["SIL",]+confmat["OVL",]+confmat["ELE",]
  labels = c("CHI", "OCH", "FEM", "MAL", "Other")
  confmat = confmat[labels, labels]
  cohen.kappa(confmat)->ck
  n_obs = ck$n.obs
  kappa = ck$kappa
  weighted_kappa = ck$weighted.kappa
  cohen[cohen$corpora == corpora, "n_obs"] = n_obs
  cohen[cohen$corpora == corpora, "kappa"] = kappa
  cohen[cohen$corpora == corpora, "weighted_kappa"] = weighted_kappa
}

cohen = cohen[order(-cohen$weighted_kappa),]
kable(cohen, row.names=FALSE) %>%
  kable_styling()
```

## Vocalizations and turn-taking

For this study, we considered only the children that have been annotated as vcm or lex, for which vocalizations were classified as C, N, W, L, U, or Y. That led us to remove 7 children from the study, for a total of 53 children.

```{r, echo=FALSE, message=FALSE}
data = read.table(file=paste0(evaldir_rel,"/key_child_voc_file_level.csv"), header=TRUE, sep=" ")

par(mfrow=c(1,2))

score = cor.test(data$gold1_CV_count, data$gold2_CV_count, method="pearson")$estimate
myrange=range(data[,c("gold1_CV_count","gold2_CV_count")],na.rm=T)
plot1 = plot(gold1_CV_count~gold2_CV_count,data=data,pch=20,col=alpha("black",.2),xlab="Human 2",ylab="Human 1",xlim=myrange,ylim=myrange, main="Level of agreement on \n CV_count") + abline(lm(gold1_CV_count~gold2_CV_count,data=data)) + abline(lm(gold1_CV_count~gold2_CV_count,data=data,subset=c(data$gold2_CV_count>0 & data$gold1_CV_count>0)),lty=2)+ mtext(paste0("Pearson's R score : ", format(score, digits=2)), side=4, col='red', adj=1)

score = cor.test(data$gold1_CNV_count, data$gold2_CNV_count, method="pearson")$estimate
myrange=range(data[,c("gold1_CNV_count","gold2_CNV_count")],na.rm=T)
plot3 = plot(gold1_CNV_count~gold2_CNV_count,data=data,pch=20,col=alpha("black",.2),xlab="Human 2",ylab="Human 1",xlim=myrange,ylim=myrange, main="Level of agreement \n on CNV_count") +
abline(lm(gold1_CNV_count~gold2_CNV_count,data=data)) +
abline(lm(gold1_CNV_count~gold2_CNV_count,data=data,subset=c(data$gold2_CNV_count>0 & data$gold1_CNV_count>0)),lty=2)+ mtext(paste0("Pearson's R score : ", format(score, digits=2)), side=4, col='red', adj=1)
```

```{r, echo=FALSE, message=FALSE}
par(mfrow=c(1,2))
score = cor.test(data$gold1_CV_cum_dur, data$gold2_CV_cum_dur, method="pearson")$estimate
myrange=range(data[,c("gold1_CV_cum_dur","gold2_CV_cum_dur")],na.rm=T)
plot1 = plot(gold1_CV_cum_dur~gold2_CV_cum_dur,data=data,pch=20,col=alpha("black",.2),xlab="Human 2",ylab="Human 1",xlim=myrange,ylim=myrange, main="Level of agreement on \n CV_cum_dur") + abline(lm(gold1_CV_cum_dur~gold2_CV_cum_dur,data=data)) + abline(lm(gold1_CV_cum_dur~gold2_CV_cum_dur,data=data,subset=c(data$gold2_CV_cum_dur>0 & data$gold1_CV_cum_dur>0)),lty=2) + mtext(paste0("Pearson's R score : ", format(score, digits=2)), side=4, col='red', adj=1)

score = cor.test(data$gold1_CNV_cum_dur, data$gold2_CNV_cum_dur, method="pearson")$estimate
myrange=range(data[,c("gold1_CNV_cum_dur","gold2_CNV_cum_dur")],na.rm=T)
plot2 = plot(gold1_CNV_cum_dur~gold2_CNV_cum_dur,data=data,pch=20,col=alpha("black",.2),xlab="Human 2",ylab="Human 1",xlim=myrange,ylim=myrange, main="Level of agreement \n on CNV_cum_dur") +
abline(lm(gold1_CNV_cum_dur~gold2_CNV_cum_dur,data=data)) +
abline(lm(gold1_CNV_cum_dur~gold2_CNV_cum_dur,data=data,subset=c(data$gold2_CNV_cum_dur>0 & data$gold1_CNV_cum_dur>0)),lty=2) + mtext(paste0("Pearson's R score : ", format(score, digits=2)), side=4, col='red', adj=1)

```

```{r, echo=FALSE, message=FALSE}
score = cor.test(data$gold1_CTC_count, data$gold2_CTC_count, method="pearson")$estimate
myrange=range(data[,c("gold1_CTC_count","gold2_CTC_count")],na.rm=T)
plot3 = plot(gold1_CTC_count~gold2_CTC_count,data=data,pch=20,col=alpha("black",.2),xlab="Human 2",ylab="Human 1",xlim=myrange,ylim=myrange, main="Level of agreement \n on CTC_count") +
abline(lm(gold1_CTC_count~gold2_CTC_count,data=data)) +
abline(lm(gold1_CTC_count~gold2_CTC_count,data=data,subset=c(data$gold2_CTC_count>0 & data$gold1_CTC_count>0)),lty=2)+ mtext(paste0("Pearson's R score : ", format(score, digits=2)), side=4, col='red', adj=1)
```
